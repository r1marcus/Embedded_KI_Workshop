{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "knowledge_distillation",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGgGXIQy9D0S"
      },
      "source": [
        "# Knowledge Distillation\n",
        "\n",
        "**Author:** [Marcus Rüb](https://www.linkedin.com/in/marcus-r%C3%BCb-3b07071b2/)<br>\n",
        "**Date created:** 2021/03/22<br>\n",
        "**Description:** Implementation of classical Knowledge Distillation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZIEytv79D0X"
      },
      "source": [
        "## Einführung in Knowledge Distillation\n",
        "\n",
        "Knowledge Distillation ist ein Verfahren zur Modellkomprimierung, bei dem ein kleines (Schüler-)Modell so trainiert wird, dass es zu einem großen vortrainierten (Lehrer-)Modell passt. Das Wissen wird vom Lehrermodell auf das Schülermodell übertragen, indem eine Verlustfunktion minimiert wird, die darauf abzielt, die aufgeweichten Logits des Lehrermodells sowie die Ground-Truth-Etiketten abzugleichen. Die Logits werden durch Anwendung einer \"Temperatur\"-Skalierungsfunktion im Softmax aufgeweicht, wodurch die Wahrscheinlichkeitsverteilung effektiv geglättet wird und die vom Lehrer gelernten Beziehungen zwischen den Klassen sichtbar werden.\n",
        "\n",
        "\n",
        "\n",
        "**Reference:**\n",
        "\n",
        "- [Hinton et al. (2015)](https://arxiv.org/abs/1503.02531)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0xUaG_C9D0X"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCpE3vmr9D0Y"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRcsJ2kf9D0Y"
      },
      "source": [
        "## Construct `Distiller()` class\n",
        "\n",
        "Die benutzerdefinierte Klasse `Distiller()` überschreibt die `Model`-Methoden `train_step`, `test_step` und `compile()`. Um den Distiller verwenden zu können, benötigen wir:\n",
        "\n",
        "- Ein trainiertes Lehrermodell\n",
        "- Ein Schüler-Modell zum Trainieren\n",
        "- Eine Schüler-Verlustfunktion für die Differenz zwischen Schüler-Vorhersagen und Groundtruth\n",
        "- Eine Destillationsverlustfunktion, zusammen mit einer `Temperatur`, auf die Differenz zwischen den weichen Studentenvorhersagen und den weichen Lehreretiketten\n",
        "- Ein \"Alpha\"-Faktor zur Gewichtung der Studenten- und Destillationsverluste\n",
        "- Ein Optimierer für den Schüler und (optionale) Metriken zur Leistungsbewertung\n",
        "\n",
        "Bei der Methode \"train_step\" führen wir einen Vorwärtsdurchlauf von Lehrer und Schüler durch, berechnen den Verlust mit der Gewichtung von \"student_loss\" und \"distillation_loss\" mit \"alpha\" bzw. \"1 - alpha\" und führen den Rückwärtsdurchlauf durch. Hinweis: Es werden nur die Schülergewichte aktualisiert, und daher werden nur die Gradienten für die Schülergewichte berechnet.\n",
        "In der Methode `test_step` wird das Studentenmodell auf dem bereitgestellten Datensatz evaluiert.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lm11W8H9D0Y"
      },
      "source": [
        "\n",
        "class Distiller(keras.Model):\n",
        "    def __init__(self, student, teacher):\n",
        "        super(Distiller, self).__init__()\n",
        "        self.teacher = teacher\n",
        "        self.student = student\n",
        "\n",
        "    def compile(\n",
        "        self,\n",
        "        optimizer,\n",
        "        metrics,\n",
        "        student_loss_fn,\n",
        "        distillation_loss_fn,\n",
        "        alpha=0.1,\n",
        "        temperature=3,\n",
        "    ):\n",
        "        \"\"\" Configure the distiller.\n",
        "\n",
        "        Args:\n",
        "            optimizer: Keras optimizer for the student weights\n",
        "            metrics: Keras metrics for evaluation\n",
        "            student_loss_fn: Loss function of difference between student\n",
        "                predictions and ground-truth\n",
        "            distillation_loss_fn: Loss function of difference between soft\n",
        "                student predictions and soft teacher predictions\n",
        "            alpha: weight to student_loss_fn and 1-alpha to distillation_loss_fn\n",
        "            temperature: Temperature for softening probability distributions.\n",
        "                Larger temperature gives softer distributions.\n",
        "        \"\"\"\n",
        "        super(Distiller, self).compile(optimizer=optimizer, metrics=metrics)\n",
        "        self.student_loss_fn = student_loss_fn\n",
        "        self.distillation_loss_fn = distillation_loss_fn\n",
        "        self.alpha = alpha\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def train_step(self, data):\n",
        "        # Unpack data\n",
        "        x, y = data\n",
        "\n",
        "        # Forward pass of teacher\n",
        "        teacher_predictions = self.teacher(x, training=False)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            # Forward pass of student\n",
        "            student_predictions = self.student(x, training=True)\n",
        "\n",
        "            # Compute losses\n",
        "            student_loss = self.student_loss_fn(y, student_predictions)\n",
        "            distillation_loss = self.distillation_loss_fn(\n",
        "                tf.nn.softmax(teacher_predictions / self.temperature, axis=1),\n",
        "                tf.nn.softmax(student_predictions / self.temperature, axis=1),\n",
        "            )\n",
        "            loss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss\n",
        "\n",
        "        # Compute gradients\n",
        "        trainable_vars = self.student.trainable_variables\n",
        "        gradients = tape.gradient(loss, trainable_vars)\n",
        "\n",
        "        # Update weights\n",
        "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "\n",
        "        # Update the metrics configured in `compile()`.\n",
        "        self.compiled_metrics.update_state(y, student_predictions)\n",
        "\n",
        "        # Return a dict of performance\n",
        "        results = {m.name: m.result() for m in self.metrics}\n",
        "        results.update(\n",
        "            {\"student_loss\": student_loss, \"distillation_loss\": distillation_loss}\n",
        "        )\n",
        "        return results\n",
        "\n",
        "    def test_step(self, data):\n",
        "        # Unpack the data\n",
        "        x, y = data\n",
        "\n",
        "        # Compute predictions\n",
        "        y_prediction = self.student(x, training=False)\n",
        "\n",
        "        # Calculate the loss\n",
        "        student_loss = self.student_loss_fn(y, y_prediction)\n",
        "\n",
        "        # Update the metrics.\n",
        "        self.compiled_metrics.update_state(y, y_prediction)\n",
        "\n",
        "        # Return a dict of performance\n",
        "        results = {m.name: m.result() for m in self.metrics}\n",
        "        results.update({\"student_loss\": student_loss})\n",
        "        return results\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXBxB6Uk9D0Z"
      },
      "source": [
        "## Create student and teacher models\n",
        "\n",
        "Zunächst erstellen wir ein Lehrer-Modell und ein kleineres Schüler-Modell. Beide Modelle sind Faltungsneuronale Netze und werden mit `Sequential()` erstellt, können aber jedes Keras-Modell sein."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KYyC9nd9D0a"
      },
      "source": [
        "# Create the teacher\n",
        "teacher = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(28, 28, 1)),\n",
        "        layers.Conv2D(256, (3, 3), strides=(2, 2), padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding=\"same\"),\n",
        "        layers.Conv2D(512, (3, 3), strides=(2, 2), padding=\"same\"),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(10),\n",
        "    ],\n",
        "    name=\"teacher\",\n",
        ")\n",
        "\n",
        "# Create the student\n",
        "student = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(28, 28, 1)),\n",
        "        layers.Conv2D(16, (3, 3), strides=(2, 2), padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding=\"same\"),\n",
        "        layers.Conv2D(32, (3, 3), strides=(2, 2), padding=\"same\"),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(10),\n",
        "    ],\n",
        "    name=\"student\",\n",
        ")\n",
        "\n",
        "# Clone student for later comparison\n",
        "student_scratch = keras.models.clone_model(student)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLJyjBVQ9D0a"
      },
      "source": [
        "## Vorbereiten des Datensatzes\n",
        "\n",
        "Der Datensatz, der für das Training des Lehrers und die Destillation des Lehrers verwendet wird, ist [MNIST](https://keras.io/api/datasets/mnist/). Das Verfahren wäre für jeden anderen Datensatz, z. B. [CIFAR-10](https://keras.io/api/datasets/cifar10/), mit einer geeigneten Wahl der Modelle äquivalent. Sowohl der Schüler als auch der Lehrer werden auf dem Trainingsset trainiert und auf dem Testset evaluiert."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s30WWt659D0b"
      },
      "source": [
        "# Prepare the train and test dataset.\n",
        "batch_size = 64\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "# Normalize data\n",
        "x_train = x_train.astype(\"float32\") / 255.0\n",
        "x_train = np.reshape(x_train, (-1, 28, 28, 1))\n",
        "\n",
        "x_test = x_test.astype(\"float32\") / 255.0\n",
        "x_test = np.reshape(x_test, (-1, 28, 28, 1))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmqZ_f2S9D0b"
      },
      "source": [
        "\n",
        "## Den Lehrer ausbilden\n",
        "\n",
        "Bei der Wissensdestillation gehen wir davon aus, dass der Lehrer trainiert und festgelegt ist. Wir beginnen also damit, das Lehrermodell auf dem Trainingsset auf die übliche Weise zu trainieren."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jb2Irzan9D0b"
      },
      "source": [
        "# Train teacher as usual\n",
        "teacher.compile(\n",
        "    optimizer=keras.optimizers.Adam(),\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
        ")\n",
        "\n",
        "# Train and evaluate teacher on data.\n",
        "teacher.fit(x_train, y_train, epochs=5)\n",
        "teacher.evaluate(x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nW6siO869D0c"
      },
      "source": [
        "## Lehrer zu Schüler destillieren\n",
        "\n",
        "Wir haben das Lehrermodell bereits trainiert und müssen nur noch eine `Distiller(student, teacher)`-Instanz initialisieren, sie mit den gewünschten Verlusten, Hyperparametern und Optimierer `kompilieren()` und den Lehrer auf den Schüler destillieren."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13pIv_1F9D0c"
      },
      "source": [
        "# Initialize and compile distiller\n",
        "distiller = Distiller(student=student, teacher=teacher)\n",
        "distiller.compile(\n",
        "    optimizer=keras.optimizers.Adam(),\n",
        "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
        "    student_loss_fn=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    distillation_loss_fn=keras.losses.KLDivergence(),\n",
        "    alpha=0.1,\n",
        "    temperature=10,\n",
        ")\n",
        "\n",
        "# Distill teacher to student\n",
        "distiller.fit(x_train, y_train, epochs=3)\n",
        "\n",
        "# Evaluate student on test dataset\n",
        "distiller.evaluate(x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ypUX7C_9D0c"
      },
      "source": [
        "## Schüler von Grund auf trainieren zum Vergleich\n",
        "\n",
        "Wir können auch ein äquivalentes Studentenmodell von Grund auf ohne den Lehrer trainieren, um den durch die Wissensdestillation erzielten Leistungsgewinn zu bewerten."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIh2tAKj9D0d"
      },
      "source": [
        "# Train student as doen usually\n",
        "student_scratch.compile(\n",
        "    optimizer=keras.optimizers.Adam(),\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
        ")\n",
        "\n",
        "# Train and evaluate student trained from scratch.\n",
        "student_scratch.fit(x_train, y_train, epochs=3)\n",
        "student_scratch.evaluate(x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-UBVpfa9D0d"
      },
      "source": [
        "Wenn der Lehrer für 5 volle Epochen trainiert wird und der Student für 3 volle Epochen auf diesem Lehrer destilliert wird, sollten Sie in diesem Beispiel einen Leistungsschub im Vergleich zum Training des gleichen Studentenmodells von Grund auf und sogar im Vergleich zum Lehrer selbst erleben. Sie sollten erwarten, dass der Lehrer eine Genauigkeit von etwa 97,6 % hat, der von Grund auf trainierte Student sollte etwa 97,6 % haben und der destillierte Student sollte etwa 98,1 % haben. Entfernen oder probieren Sie verschiedene Seeds aus, um unterschiedliche Gewichtungsinitialisierungen zu verwenden."
      ]
    }
  ]
}