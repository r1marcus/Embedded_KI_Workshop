{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pruning_Workshop.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPtZxbLX8qou2rtRG3+55Gr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/r1marcus/Embedded_KI_Workshop/blob/main/Pruning_Workshop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gV93-URh81q3"
      },
      "source": [
        "# Introducing Pruning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6ICvyya84Yo"
      },
      "source": [
        "!pip install -q tensorflow-model-optimization"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QD5dB3zo9Dn9",
        "outputId": "938c5257-e6a9-4e0c-cdf4-8d961bd1e8f8"
      },
      "source": [
        "import tensorflow\r\n",
        "from tensorflow.keras.datasets import mnist\r\n",
        "from tensorflow.keras.models import Sequential\r\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten\r\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\r\n",
        "import tempfile\r\n",
        "#import tensorflow_model_optimization as tfmot\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "\r\n",
        "# Model configuration\r\n",
        "img_width, img_height = 28, 28\r\n",
        "batch_size = 250\r\n",
        "no_epochs = 10\r\n",
        "no_classes = 10\r\n",
        "validation_split = 0.2\r\n",
        "verbosity = 1\r\n",
        "\r\n",
        "# Load MNIST dataset\r\n",
        "(input_train, target_train), (input_test, target_test) = mnist.load_data()\r\n",
        "input_shape = (img_width, img_height, 1)\r\n",
        "\r\n",
        "# Reshape data for ConvNet\r\n",
        "input_train = input_train.reshape(input_train.shape[0], img_width, img_height, 1)\r\n",
        "input_test = input_test.reshape(input_test.shape[0], img_width, img_height, 1)\r\n",
        "input_shape = (img_width, img_height, 1)\r\n",
        "\r\n",
        "# Parse numbers as floats\r\n",
        "input_train = input_train.astype('float32')\r\n",
        "input_test = input_test.astype('float32')\r\n",
        "\r\n",
        "# Normalize [0, 255] into [0, 1]\r\n",
        "input_train = input_train / 255\r\n",
        "input_test = input_test / 255\r\n",
        "\r\n",
        "# Convert target vectors to categorical targets\r\n",
        "target_train = tensorflow.keras.utils.to_categorical(target_train, no_classes)\r\n",
        "target_test = tensorflow.keras.utils.to_categorical(target_test, no_classes)\r\n",
        "\r\n",
        "# Create the model\r\n",
        "model = Sequential()\r\n",
        "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\r\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\r\n",
        "model.add(Dropout(0.25))\r\n",
        "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\r\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\r\n",
        "model.add(Dropout(0.25))\r\n",
        "model.add(Flatten())\r\n",
        "model.add(Dense(256, activation='relu'))\r\n",
        "model.add(Dense(no_classes, activation='softmax'))\r\n",
        "\r\n",
        "# Compile the model\r\n",
        "model.compile(loss=tensorflow.keras.losses.categorical_crossentropy,\r\n",
        "              optimizer=tensorflow.keras.optimizers.Adam(),\r\n",
        "              metrics=['accuracy'])\r\n",
        "\r\n",
        "# Fit data to model\r\n",
        "model.fit(input_train, target_train,\r\n",
        "          batch_size=batch_size,\r\n",
        "          epochs=no_epochs,\r\n",
        "          verbose=verbosity,\r\n",
        "          validation_split=validation_split)\r\n",
        "\r\n",
        "# Generate generalization metrics\r\n",
        "score = model.evaluate(input_test, target_test, verbose=0)\r\n",
        "print(f'Test loss: {score[0]} / Test accuracy: {score[1]}')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "192/192 [==============================] - 34s 176ms/step - loss: 0.7451 - accuracy: 0.7749 - val_loss: 0.1040 - val_accuracy: 0.9722\n",
            "Epoch 2/10\n",
            "192/192 [==============================] - 33s 174ms/step - loss: 0.1063 - accuracy: 0.9668 - val_loss: 0.0630 - val_accuracy: 0.9822\n",
            "Epoch 3/10\n",
            "192/192 [==============================] - 34s 175ms/step - loss: 0.0702 - accuracy: 0.9785 - val_loss: 0.0474 - val_accuracy: 0.9873\n",
            "Epoch 4/10\n",
            "192/192 [==============================] - 34s 175ms/step - loss: 0.0533 - accuracy: 0.9837 - val_loss: 0.0392 - val_accuracy: 0.9890\n",
            "Epoch 5/10\n",
            "192/192 [==============================] - 33s 174ms/step - loss: 0.0441 - accuracy: 0.9860 - val_loss: 0.0389 - val_accuracy: 0.9886\n",
            "Epoch 6/10\n",
            "192/192 [==============================] - 33s 173ms/step - loss: 0.0365 - accuracy: 0.9884 - val_loss: 0.0436 - val_accuracy: 0.9870\n",
            "Epoch 7/10\n",
            "192/192 [==============================] - 33s 174ms/step - loss: 0.0330 - accuracy: 0.9894 - val_loss: 0.0325 - val_accuracy: 0.9906\n",
            "Epoch 8/10\n",
            "192/192 [==============================] - 33s 174ms/step - loss: 0.0270 - accuracy: 0.9921 - val_loss: 0.0346 - val_accuracy: 0.9895\n",
            "Epoch 9/10\n",
            "192/192 [==============================] - 33s 173ms/step - loss: 0.0266 - accuracy: 0.9917 - val_loss: 0.0297 - val_accuracy: 0.9917\n",
            "Epoch 10/10\n",
            "192/192 [==============================] - 33s 172ms/step - loss: 0.0215 - accuracy: 0.9931 - val_loss: 0.0372 - val_accuracy: 0.9888\n",
            "Test loss: 0.03263925760984421 / Test accuracy: 0.9894999861717224\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xW8DnwDN-7k-"
      },
      "source": [
        "# Modell Speichern\r\n",
        "Stellen Sie außerdem sicher, dass Sie Ihr Modell in einer temporären Datei speichern, so dass Sie später die Größen des ursprünglichen und des geprunten Modells vergleichen können:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ICuQ3bMx-7zX",
        "outputId": "fcffa1ce-f407-4167-fee8-a37eb519521a"
      },
      "source": [
        "\r\n",
        "from tensorflow import keras\r\n",
        "import tensorflow_model_optimization as tfmot\r\n",
        "_, keras_file = tempfile.mkstemp('.h5')\r\n",
        "keras.models.save_model(model, keras_file, include_optimizer=False)\r\n",
        "print(f'Baseline model saved: {keras_file}')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Baseline model saved: /tmp/tmpbz26u9a3.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlRkmC4cFeRd"
      },
      "source": [
        "# Loading and configuring pruning\r\n",
        "\r\n",
        "Es lädt die prune_low_magnitude Funktionalität von TensorFlow (Tfmot.sparsity.keras.prune_low_magnitude, n.d.). prune_low_magnitude modifiziert einfach einen Layer, indem es ihn für das Pruning bereit macht. Dies geschieht, indem ein Keras-Modell mit Pruning-Funktionalität umhüllt wird, genauer gesagt, indem sichergestellt wird, dass die Schichten des Modells beschneidbar sind. Damit wird nur die Funktionalität geladen, wir werden sie später tatsächlich aufrufen.\r\n",
        "\r\n",
        "Nach dem Laden des Pruning-Wrappers werden wir die Pruning-Konfiguration festlegen:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dt4LRfrWFc-7",
        "outputId": "f82d813c-72dc-489b-9f57-88efa29b517a"
      },
      "source": [
        "# Finish pruning after 5 epochs\r\n",
        "pruning_epochs = 5\r\n",
        "num_images = input_train.shape[0] * (1 - validation_split)\r\n",
        "end_step = np.ceil(num_images / batch_size).astype(np.int32) * pruning_epochs\r\n",
        "\r\n",
        "# Define pruning configuration\r\n",
        "pruning_params = {\r\n",
        "      'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.40,\r\n",
        "                                                               final_sparsity=0.70,\r\n",
        "                                                               begin_step=0,\r\n",
        "                                                               end_step=end_step)\r\n",
        "}\r\n",
        "model_for_pruning = tfmot.sparsity.keras.prune_low_magnitude(model, **pruning_params)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py:2281: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  warnings.warn('`layer.add_variable` is deprecated and '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTY3KYAOF7Wn"
      },
      "source": [
        "Hier geschieht Folgendes:\r\n",
        "\r\n",
        "Wir konfigurieren die Länge des Pruning-Prozesses über die Anzahl der Epochen, für die das Modell prunen soll, und nehmen eine Feinabstimmung vor.\r\n",
        "Wir laden die Anzahl der Bilder, die in unserem Trainingssatz verwendet werden, abzüglich der Validierungsdaten.\r\n",
        "Wir berechnen den end_step unseres Pruning-Prozesses anhand der Stapelgröße, der Anzahl der Bilder sowie der Anzahl der Epochen.\r\n",
        "Anschließend definieren wir die Konfiguration für den Pruning-Vorgang über pruning_params. Wir definieren einen Pruning-Zeitplan unter Verwendung von PolynomialDecay, was bedeutet, dass die Sparsamkeit des Modells mit zunehmender Anzahl von Epochen zunimmt. Zu Beginn wird das Modell auf 40 % Sparsamkeit eingestellt und wird zunehmend spärlicher, bis es schließlich 70 % erreicht. Wir beginnen bei 0 und enden bei end_step.\r\n",
        "Schließlich rufen wir die Funktion \"prune_low_magnitude\" (die das beschneidbare Modell erzeugt) mit unserem Ausgangsmodell und den definierten \"pruning_params\" auf.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwYDd--5Hx84"
      },
      "source": [
        "# Starten des Beschneidungsvorgangs\r\n",
        "Nachdem wir den Pruning-Prozess konfiguriert haben, können wir das Modell neu kompilieren (dies ist notwendig, da wir die Pruning-Funktionalität hinzugefügt haben) und den Pruning-Prozess starten. Wir müssen hier den UpdatePruningStep-Callback verwenden, da er die Optimierungsaktivitäten an den Pruning-Prozess weitergibt (Tfmot.sparsity.keras.UpdatePruningStep, o.J.)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WITZfM4VF7lH",
        "outputId": "f5c8bc39-a793-49ed-adef-094f771e9cc7"
      },
      "source": [
        "# Recompile the model\r\n",
        "model_for_pruning.compile(loss=tensorflow.keras.losses.categorical_crossentropy,\r\n",
        "              optimizer=tensorflow.keras.optimizers.Adam(),\r\n",
        "              metrics=['accuracy'])\r\n",
        "\r\n",
        "# Model callbacks\r\n",
        "callbacks = [\r\n",
        "  tfmot.sparsity.keras.UpdatePruningStep()\r\n",
        "]\r\n",
        "\r\n",
        "# Fitting data\r\n",
        "model_for_pruning.fit(input_train, target_train,\r\n",
        "                      batch_size=batch_size,\r\n",
        "                      epochs=pruning_epochs,\r\n",
        "                      verbose=verbosity,\r\n",
        "                      callbacks=callbacks,\r\n",
        "                      validation_split=validation_split)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "192/192 [==============================] - 36s 176ms/step - loss: 0.0219 - accuracy: 0.9926 - val_loss: 0.0332 - val_accuracy: 0.9898\n",
            "Epoch 2/5\n",
            "192/192 [==============================] - 34s 176ms/step - loss: 0.0252 - accuracy: 0.9920 - val_loss: 0.0290 - val_accuracy: 0.9915\n",
            "Epoch 3/5\n",
            "192/192 [==============================] - 34s 175ms/step - loss: 0.0218 - accuracy: 0.9928 - val_loss: 0.0274 - val_accuracy: 0.9924\n",
            "Epoch 4/5\n",
            "192/192 [==============================] - 34s 177ms/step - loss: 0.0173 - accuracy: 0.9945 - val_loss: 0.0277 - val_accuracy: 0.9926\n",
            "Epoch 5/5\n",
            "192/192 [==============================] - 34s 177ms/step - loss: 0.0150 - accuracy: 0.9954 - val_loss: 0.0259 - val_accuracy: 0.9929\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f90102dae48>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NWwuOtwGHoJ"
      },
      "source": [
        "# Messen der Effektivität des Beschneidens\r\n",
        "Sobald das Pruning abgeschlossen ist, müssen wir seine Effektivität messen. Das können wir auf zwei Arten tun:\r\n",
        "\r\n",
        "Indem man misst, wie sehr sich die Leistung im Vergleich zu vor dem Pruning verändert hat;\r\n",
        "Indem wir messen, wie sehr sich die Modellgröße im Vergleich zur Zeit vor dem Pruning verändert hat.\r\n",
        "Wir tun dies mit den folgenden Codezeilen:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EPz1lntIGH2X",
        "outputId": "b1aafb83-a7fb-41c2-d33f-f09396926ee7"
      },
      "source": [
        "# Generate generalization metrics\r\n",
        "score_pruned = model_for_pruning.evaluate(input_test, target_test, verbose=0)\r\n",
        "print(f'Pruned CNN - Test loss: {score_pruned[0]} / Test accuracy: {score_pruned[1]}')\r\n",
        "print(f'Regular CNN - Test loss: {score[0]} / Test accuracy: {score[1]}')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pruned CNN - Test loss: 0.022952646017074585 / Test accuracy: 0.9922999739646912\n",
            "Regular CNN - Test loss: 0.03263925760984421 / Test accuracy: 0.9894999861717224\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Q8lCAk0GNNq"
      },
      "source": [
        "Diese sind einfach. Sie werten das beschnittene Modell mit den Testdaten aus und geben anschließend das Ergebnis aus, ebenso wie das (zuvor erhaltene) Ergebnis des ursprünglichen Modells.\r\n",
        "\r\n",
        "Als Nächstes exportieren wir es erneut - genau wie zuvor - um sicherzustellen, dass wir es vergleichen können:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5e-VwzowGNmy",
        "outputId": "aee4f7a5-8eec-4159-d5c5-07793bc1e901"
      },
      "source": [
        "# Export the model\r\n",
        "model_for_export = tfmot.sparsity.keras.strip_pruning(model_for_pruning)\r\n",
        "_, pruned_keras_file = tempfile.mkstemp('.h5')\r\n",
        "keras.models.save_model(model_for_export, pruned_keras_file, include_optimizer=False)\r\n",
        "print(f'Pruned model saved: {keras_file}')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pruned model saved: /tmp/tmpbz26u9a3.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BiPdaJCMGSab"
      },
      "source": [
        "Anschließend können wir (dank Pruning Keras Example (n.d.)) die Größe des Keras-Modells vergleichen. Um die Vorteile von Pruning zu verdeutlichen, müssen wir einen Komprimierungsalgorithmus wie gzip verwenden, wonach wir die Größen beider Modelle vergleichen können. Erinnern Sie sich daran, dass Pruning Sparsity erzeugt und dass dünn besetzte Matrizen bei der Komprimierung sehr effizient gespeichert werden können. Deshalb ist gzips für Demonstrationszwecke nützlich. Wir erstellen zunächst eine def, die für die Komprimierung verwendet werden kann, und rufen sie anschließend zweimal auf:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nMchpvQkGSnf",
        "outputId": "31d1ae52-56df-4b1b-8832-d2ad5f1a1b57"
      },
      "source": [
        "# Measuring the size of your pruned model\r\n",
        "# (source: https://www.tensorflow.org/model_optimization/guide/pruning/pruning_with_keras#fine-tune_pre-trained_model_with_pruning)\r\n",
        "\r\n",
        "def get_gzipped_model_size(file):\r\n",
        "  # Returns size of gzipped model, in bytes.\r\n",
        "  import os\r\n",
        "  import zipfile\r\n",
        "\r\n",
        "  _, zipped_file = tempfile.mkstemp('.zip')\r\n",
        "  with zipfile.ZipFile(zipped_file, 'w', compression=zipfile.ZIP_DEFLATED) as f:\r\n",
        "    f.write(file)\r\n",
        "\r\n",
        "  return os.path.getsize(zipped_file)\r\n",
        "\r\n",
        "print(\"Size of gzipped baseline Keras model: %.2f bytes\" % (get_gzipped_model_size(keras_file)))\r\n",
        "print(\"Size of gzipped pruned Keras model: %.2f bytes\" % (get_gzipped_model_size(pruned_keras_file)))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of gzipped baseline Keras model: 1601298.00 bytes\n",
            "Size of gzipped pruned Keras model: 679955.00 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cddAfnhGTOk"
      },
      "source": [
        "# Laufzeit-Ergebnis\r\n",
        "Nun ist es an der Zeit, es auszuführen. Speichern Sie Ihre Datei als z.B. pruning.py, und führen Sie sie in einer Python-Umgebung aus, in der Sie sowohl tensorflow 2.x als auch numpy und das tensorflow_model_optimization toolkit installiert haben.\r\n",
        "\r\n",
        "Zuerst wird das reguläre Training gestartet, gefolgt vom Pruning-Prozess, und dann wird die Effektivität auf dem Bildschirm angezeigt. Zunächst in Bezug auf die Modellleistung (d. h. Verlust und Genauigkeit):\r\n",
        "\r\n",
        "Pruned CNN - Testverlust: 0.0218335362634185 / Testgenauigkeit: 0.9923999905586243\r\n",
        "Regulärer CNN - Testverlust: 0.02442687187876436 / Testgenauigkeit: 0.9915000200271606\r\n",
        "Das beschnittene Modell schneidet sogar etwas besser ab als das reguläre Modell. Das liegt wahrscheinlich daran, dass wir das ursprüngliche Modell nur für 10 Epochen trainiert haben und danach mit dem Pruning fortgefahren sind. Es ist sehr gut möglich, dass das Modell noch nicht konvergiert war, sondern sich im Pruning-Prozess weiter in Richtung Konvergenz bewegt hat. Oftmals verschlechtert sich die Performance ein wenig, aber das sollte nur geringfügig sein.\r\n",
        "\r\n",
        "Dann, in Bezug auf die Modellgröße:\r\n",
        "\r\n",
        "Größe des gezippten Keras-Basismodells: 1601609.00 Bytes\r\n",
        "Größe des gzipped pruned Keras-Modells: 679958.00 bytes\r\n",
        "Code-Sprache: CSS (css)\r\n",
        "Das Pruning hat unser Modell definitiv kleiner gemacht - 2,35 Mal!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gnSP5F4GTYj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}