{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pruning_Workshop.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNb871JznBf8cidFhgfPsHl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/r1marcus/Embedded_KI_Workshop/blob/main/Pruning_Workshop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gV93-URh81q3"
      },
      "source": [
        "# Introducing Pruning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_6ICvyya84Yo",
        "outputId": "b4e53048-5160-4ddf-8330-cb02b8091cb0"
      },
      "source": [
        "!pip install -q tensorflow-model-optimization"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |██                              | 10kB 17.9MB/s eta 0:00:01\r\u001b[K     |███▉                            | 20kB 8.0MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 30kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 40kB 7.5MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 51kB 5.2MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 61kB 5.4MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 71kB 5.4MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 81kB 5.5MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 92kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 102kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 112kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 122kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 133kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 143kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 153kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 163kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 174kB 5.7MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QD5dB3zo9Dn9",
        "outputId": "d7eb5c64-fcef-4ec9-c389-7dc9bbc836b7"
      },
      "source": [
        "import tensorflow\r\n",
        "from tensorflow.keras.datasets import mnist\r\n",
        "from tensorflow.keras.models import Sequential\r\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten\r\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\r\n",
        "import tempfile\r\n",
        "#import tensorflow_model_optimization as tfmot\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "\r\n",
        "# Model configuration\r\n",
        "img_width, img_height = 28, 28\r\n",
        "batch_size = 250\r\n",
        "no_epochs = 10\r\n",
        "no_classes = 10\r\n",
        "validation_split = 0.2\r\n",
        "verbosity = 1\r\n",
        "\r\n",
        "# Load MNIST dataset\r\n",
        "(input_train, target_train), (input_test, target_test) = mnist.load_data()\r\n",
        "input_shape = (img_width, img_height, 1)\r\n",
        "\r\n",
        "# Reshape data for ConvNet\r\n",
        "input_train = input_train.reshape(input_train.shape[0], img_width, img_height, 1)\r\n",
        "input_test = input_test.reshape(input_test.shape[0], img_width, img_height, 1)\r\n",
        "input_shape = (img_width, img_height, 1)\r\n",
        "\r\n",
        "# Parse numbers as floats\r\n",
        "input_train = input_train.astype('float32')\r\n",
        "input_test = input_test.astype('float32')\r\n",
        "\r\n",
        "# Normalize [0, 255] into [0, 1]\r\n",
        "input_train = input_train / 255\r\n",
        "input_test = input_test / 255\r\n",
        "\r\n",
        "# Convert target vectors to categorical targets\r\n",
        "target_train = tensorflow.keras.utils.to_categorical(target_train, no_classes)\r\n",
        "target_test = tensorflow.keras.utils.to_categorical(target_test, no_classes)\r\n",
        "\r\n",
        "# Create the model\r\n",
        "model = Sequential()\r\n",
        "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\r\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\r\n",
        "model.add(Dropout(0.25))\r\n",
        "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\r\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\r\n",
        "model.add(Dropout(0.25))\r\n",
        "model.add(Flatten())\r\n",
        "model.add(Dense(256, activation='relu'))\r\n",
        "model.add(Dense(no_classes, activation='softmax'))\r\n",
        "\r\n",
        "# Compile the model\r\n",
        "model.compile(loss=tensorflow.keras.losses.categorical_crossentropy,\r\n",
        "              optimizer=tensorflow.keras.optimizers.Adam(),\r\n",
        "              metrics=['accuracy'])\r\n",
        "\r\n",
        "# Fit data to model\r\n",
        "model.fit(input_train, target_train,\r\n",
        "          batch_size=batch_size,\r\n",
        "          epochs=no_epochs,\r\n",
        "          verbose=verbosity,\r\n",
        "          validation_split=validation_split)\r\n",
        "\r\n",
        "# Generate generalization metrics\r\n",
        "score = model.evaluate(input_test, target_test, verbose=0)\r\n",
        "print(f'Test loss: {score[0]} / Test accuracy: {score[1]}')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "Epoch 1/10\n",
            "192/192 [==============================] - 40s 204ms/step - loss: 0.6938 - accuracy: 0.7977 - val_loss: 0.0979 - val_accuracy: 0.9718\n",
            "Epoch 2/10\n",
            "192/192 [==============================] - 39s 202ms/step - loss: 0.1021 - accuracy: 0.9678 - val_loss: 0.0611 - val_accuracy: 0.9834\n",
            "Epoch 3/10\n",
            "192/192 [==============================] - 39s 203ms/step - loss: 0.0720 - accuracy: 0.9771 - val_loss: 0.0485 - val_accuracy: 0.9863\n",
            "Epoch 4/10\n",
            "192/192 [==============================] - 39s 203ms/step - loss: 0.0577 - accuracy: 0.9813 - val_loss: 0.0431 - val_accuracy: 0.9879\n",
            "Epoch 5/10\n",
            "192/192 [==============================] - 39s 202ms/step - loss: 0.0432 - accuracy: 0.9868 - val_loss: 0.0421 - val_accuracy: 0.9885\n",
            "Epoch 6/10\n",
            "192/192 [==============================] - 39s 201ms/step - loss: 0.0372 - accuracy: 0.9882 - val_loss: 0.0388 - val_accuracy: 0.9896\n",
            "Epoch 7/10\n",
            "192/192 [==============================] - 39s 202ms/step - loss: 0.0339 - accuracy: 0.9892 - val_loss: 0.0349 - val_accuracy: 0.9898\n",
            "Epoch 8/10\n",
            "192/192 [==============================] - 39s 201ms/step - loss: 0.0273 - accuracy: 0.9913 - val_loss: 0.0314 - val_accuracy: 0.9903\n",
            "Epoch 9/10\n",
            "192/192 [==============================] - 44s 232ms/step - loss: 0.0260 - accuracy: 0.9919 - val_loss: 0.0341 - val_accuracy: 0.9900\n",
            "Epoch 10/10\n",
            "192/192 [==============================] - 38s 200ms/step - loss: 0.0218 - accuracy: 0.9931 - val_loss: 0.0329 - val_accuracy: 0.9906\n",
            "Test loss: 0.023596348240971565 / Test accuracy: 0.9914000034332275\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xW8DnwDN-7k-"
      },
      "source": [
        "# Modell Speichern\r\n",
        "Stellen Sie außerdem sicher, dass Sie Ihr Modell in einer temporären Datei speichern, so dass Sie später die Größen des ursprünglichen und des geprunten Modells vergleichen können:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ICuQ3bMx-7zX",
        "outputId": "d8898a81-9335-4c1d-ac7c-0fbb0eac09de"
      },
      "source": [
        "\r\n",
        "from tensorflow import keras\r\n",
        "import tensorflow_model_optimization as tfmot\r\n",
        "_, keras_file = tempfile.mkstemp('.h5')\r\n",
        "keras.models.save_model(model, keras_file, include_optimizer=False)\r\n",
        "print(f'Baseline model saved: {keras_file}')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Baseline model saved: /tmp/tmp2jfznccm.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlRkmC4cFeRd"
      },
      "source": [
        "# Loading and configuring pruning\r\n",
        "\r\n",
        "Es lädt die prune_low_magnitude Funktionalität von TensorFlow (Tfmot.sparsity.keras.prune_low_magnitude, n.d.). prune_low_magnitude modifiziert einfach einen Layer, indem es ihn für das Pruning bereit macht. Dies geschieht, indem ein Keras-Modell mit Pruning-Funktionalität umhüllt wird, genauer gesagt, indem sichergestellt wird, dass die Schichten des Modells beschneidbar sind. Damit wird nur die Funktionalität geladen, wir werden sie später tatsächlich aufrufen.\r\n",
        "\r\n",
        "Nach dem Laden des Pruning-Wrappers werden wir die Pruning-Konfiguration festlegen:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dt4LRfrWFc-7",
        "outputId": "560415ea-99cc-4958-9a9b-444160b4baa2"
      },
      "source": [
        "# Finish pruning after 5 epochs\r\n",
        "pruning_epochs = 5\r\n",
        "num_images = input_train.shape[0] * (1 - validation_split)\r\n",
        "end_step = np.ceil(num_images / batch_size).astype(np.int32) * pruning_epochs\r\n",
        "\r\n",
        "# Define pruning configuration\r\n",
        "pruning_params = {\r\n",
        "      'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.40,\r\n",
        "                                                               final_sparsity=0.70,\r\n",
        "                                                               begin_step=0,\r\n",
        "                                                               end_step=end_step)\r\n",
        "}\r\n",
        "model_for_pruning = tfmot.sparsity.keras.prune_low_magnitude(model, **pruning_params)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py:2281: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  warnings.warn('`layer.add_variable` is deprecated and '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTY3KYAOF7Wn"
      },
      "source": [
        "Hier geschieht Folgendes:\r\n",
        "\r\n",
        "Wir konfigurieren die Länge des Pruning-Prozesses über die Anzahl der Epochen, für die das Modell prunen soll, und nehmen eine Feinabstimmung vor.\r\n",
        "Wir laden die Anzahl der Bilder, die in unserem Trainingssatz verwendet werden, abzüglich der Validierungsdaten.\r\n",
        "Wir berechnen den end_step unseres Pruning-Prozesses anhand der Stapelgröße, der Anzahl der Bilder sowie der Anzahl der Epochen.\r\n",
        "Anschließend definieren wir die Konfiguration für den Pruning-Vorgang über pruning_params. Wir definieren einen Pruning-Zeitplan unter Verwendung von PolynomialDecay, was bedeutet, dass die Sparsamkeit des Modells mit zunehmender Anzahl von Epochen zunimmt. Zu Beginn wird das Modell auf 40 % Sparsamkeit eingestellt und wird zunehmend spärlicher, bis es schließlich 70 % erreicht. Wir beginnen bei 0 und enden bei end_step.\r\n",
        "Schließlich rufen wir die Funktion \"prune_low_magnitude\" (die das beschneidbare Modell erzeugt) mit unserem Ausgangsmodell und den definierten \"pruning_params\" auf.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwYDd--5Hx84"
      },
      "source": [
        "# Starten des Beschneidungsvorgangs\r\n",
        "Nachdem wir den Pruning-Prozess konfiguriert haben, können wir das Modell neu kompilieren (dies ist notwendig, da wir die Pruning-Funktionalität hinzugefügt haben) und den Pruning-Prozess starten. Wir müssen hier den UpdatePruningStep-Callback verwenden, da er die Optimierungsaktivitäten an den Pruning-Prozess weitergibt (Tfmot.sparsity.keras.UpdatePruningStep, o.J.)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WITZfM4VF7lH",
        "outputId": "ad590d2c-4937-405d-9bcd-0a8f77e24780"
      },
      "source": [
        "# Recompile the model\r\n",
        "model_for_pruning.compile(loss=tensorflow.keras.losses.categorical_crossentropy,\r\n",
        "              optimizer=tensorflow.keras.optimizers.Adam(),\r\n",
        "              metrics=['accuracy'])\r\n",
        "\r\n",
        "# Model callbacks\r\n",
        "callbacks = [\r\n",
        "  tfmot.sparsity.keras.UpdatePruningStep()\r\n",
        "]\r\n",
        "\r\n",
        "# Fitting data\r\n",
        "model_for_pruning.fit(input_train, target_train,\r\n",
        "                      batch_size=batch_size,\r\n",
        "                      epochs=pruning_epochs,\r\n",
        "                      verbose=verbosity,\r\n",
        "                      callbacks=callbacks,\r\n",
        "                      validation_split=validation_split)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "192/192 [==============================] - 42s 205ms/step - loss: 0.0214 - accuracy: 0.9931 - val_loss: 0.0292 - val_accuracy: 0.9911\n",
            "Epoch 2/5\n",
            "192/192 [==============================] - 40s 207ms/step - loss: 0.0247 - accuracy: 0.9918 - val_loss: 0.0295 - val_accuracy: 0.9913\n",
            "Epoch 3/5\n",
            "192/192 [==============================] - 40s 206ms/step - loss: 0.0209 - accuracy: 0.9931 - val_loss: 0.0282 - val_accuracy: 0.9923\n",
            "Epoch 4/5\n",
            "192/192 [==============================] - 40s 206ms/step - loss: 0.0185 - accuracy: 0.9946 - val_loss: 0.0299 - val_accuracy: 0.9916\n",
            "Epoch 5/5\n",
            "192/192 [==============================] - 39s 205ms/step - loss: 0.0158 - accuracy: 0.9954 - val_loss: 0.0275 - val_accuracy: 0.9920\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f50aa4cb3c8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NWwuOtwGHoJ"
      },
      "source": [
        "# Messen der Effektivität des Beschneidens\r\n",
        "Sobald das Pruning abgeschlossen ist, müssen wir seine Effektivität messen. Das können wir auf zwei Arten tun:\r\n",
        "\r\n",
        "Indem man misst, wie sehr sich die Leistung im Vergleich zu vor dem Pruning verändert hat;\r\n",
        "Indem wir messen, wie sehr sich die Modellgröße im Vergleich zur Zeit vor dem Pruning verändert hat.\r\n",
        "Wir tun dies mit den folgenden Codezeilen:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EPz1lntIGH2X",
        "outputId": "5fcf6b26-294e-40d6-bfe2-65b688669738"
      },
      "source": [
        "# Generate generalization metrics\r\n",
        "score_pruned = model_for_pruning.evaluate(input_test, target_test, verbose=0)\r\n",
        "print(f'Pruned CNN - Test loss: {score_pruned[0]} / Test accuracy: {score_pruned[1]}')\r\n",
        "print(f'Regular CNN - Test loss: {score[0]} / Test accuracy: {score[1]}')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pruned CNN - Test loss: 0.021241622045636177 / Test accuracy: 0.9926000237464905\n",
            "Regular CNN - Test loss: 0.023596348240971565 / Test accuracy: 0.9914000034332275\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Q8lCAk0GNNq"
      },
      "source": [
        "Diese sind einfach. Sie werten das beschnittene Modell mit den Testdaten aus und geben anschließend das Ergebnis aus, ebenso wie das (zuvor erhaltene) Ergebnis des ursprünglichen Modells.\r\n",
        "\r\n",
        "Als Nächstes exportieren wir es erneut - genau wie zuvor - um sicherzustellen, dass wir es vergleichen können:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5e-VwzowGNmy",
        "outputId": "c01a47f3-358a-425a-8e27-9231e93e094a"
      },
      "source": [
        "# Export the model\r\n",
        "model_for_export = tfmot.sparsity.keras.strip_pruning(model_for_pruning)\r\n",
        "_, pruned_keras_file = tempfile.mkstemp('.h5')\r\n",
        "keras.models.save_model(model_for_export, pruned_keras_file, include_optimizer=False)\r\n",
        "print(f'Pruned model saved: {keras_file}')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pruned model saved: /tmp/tmp2jfznccm.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BiPdaJCMGSab"
      },
      "source": [
        "Anschließend können wir (dank Pruning Keras Example (n.d.)) die Größe des Keras-Modells vergleichen. Um die Vorteile von Pruning zu verdeutlichen, müssen wir einen Komprimierungsalgorithmus wie gzip verwenden, wonach wir die Größen beider Modelle vergleichen können. Erinnern Sie sich daran, dass Pruning Sparsity erzeugt und dass dünn besetzte Matrizen bei der Komprimierung sehr effizient gespeichert werden können. Deshalb ist gzips für Demonstrationszwecke nützlich. Wir erstellen zunächst eine def, die für die Komprimierung verwendet werden kann, und rufen sie anschließend zweimal auf:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nMchpvQkGSnf",
        "outputId": "873bdd1e-d431-4670-c4d3-f6f860182b98"
      },
      "source": [
        "# Measuring the size of your pruned model\r\n",
        "# (source: https://www.tensorflow.org/model_optimization/guide/pruning/pruning_with_keras#fine-tune_pre-trained_model_with_pruning)\r\n",
        "\r\n",
        "def get_gzipped_model_size(file):\r\n",
        "  # Returns size of gzipped model, in bytes.\r\n",
        "  import os\r\n",
        "  import zipfile\r\n",
        "\r\n",
        "  _, zipped_file = tempfile.mkstemp('.zip')\r\n",
        "  with zipfile.ZipFile(zipped_file, 'w', compression=zipfile.ZIP_DEFLATED) as f:\r\n",
        "    f.write(file)\r\n",
        "\r\n",
        "  return os.path.getsize(zipped_file)\r\n",
        "\r\n",
        "print(\"Size of gzipped baseline Keras model: %.2f bytes\" % (get_gzipped_model_size(keras_file)))\r\n",
        "print(\"Size of gzipped pruned Keras model: %.2f bytes\" % (get_gzipped_model_size(pruned_keras_file)))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of gzipped baseline Keras model: 1601249.00 bytes\n",
            "Size of gzipped pruned Keras model: 679469.00 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cddAfnhGTOk"
      },
      "source": [
        "# Laufzeit-Ergebnis\r\n",
        "Nun ist es an der Zeit, es auszuführen. Speichern Sie Ihre Datei als z.B. pruning.py, und führen Sie sie in einer Python-Umgebung aus, in der Sie sowohl tensorflow 2.x als auch numpy und das tensorflow_model_optimization toolkit installiert haben.\r\n",
        "\r\n",
        "Zuerst wird das reguläre Training gestartet, gefolgt vom Pruning-Prozess, und dann wird die Effektivität auf dem Bildschirm angezeigt. Zunächst in Bezug auf die Modellleistung (d. h. Verlust und Genauigkeit):\r\n",
        "\r\n",
        "Pruned CNN - Testverlust: 0.0218335362634185 / Testgenauigkeit: 0.9923999905586243\r\n",
        "Regulärer CNN - Testverlust: 0.02442687187876436 / Testgenauigkeit: 0.9915000200271606\r\n",
        "Das beschnittene Modell schneidet sogar etwas besser ab als das reguläre Modell. Das liegt wahrscheinlich daran, dass wir das ursprüngliche Modell nur für 10 Epochen trainiert haben und danach mit dem Pruning fortgefahren sind. Es ist sehr gut möglich, dass das Modell noch nicht konvergiert war, sondern sich im Pruning-Prozess weiter in Richtung Konvergenz bewegt hat. Oftmals verschlechtert sich die Performance ein wenig, aber das sollte nur geringfügig sein.\r\n",
        "\r\n",
        "Dann, in Bezug auf die Modellgröße:\r\n",
        "\r\n",
        "Größe des gezippten Keras-Basismodells: 1601609.00 Bytes\r\n",
        "Größe des gzipped pruned Keras-Modells: 679958.00 bytes\r\n",
        "Code-Sprache: CSS (css)\r\n",
        "Das Pruning hat unser Modell definitiv kleiner gemacht - 2,35 Mal!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RopkgyriuaEh"
      },
      "source": [
        "# Was ist Quantisierung?\r\n",
        "Quantisierung, kurz gesagt, bedeutet, die Zahlendarstellung Ihres Machine-Learning-Modells zu ändern (ob das Gewichte oder auch Aktivierungen sind), um sie kleiner zu machen.\r\n",
        "\r\n",
        "Standardmäßig arbeiten TensorFlow und Keras mit dem float32-Format. Mit 32-Bit Fließkommazahlen ist es möglich, wirklich große Zahlen mit großer Präzision zu speichern. Allerdings macht die Tatsache, dass 32 Bit verwendet werden können, das Modell nicht so effizient in Bezug auf die Speicherung - und auch nicht in Bezug auf die Geschwindigkeit (Float-Operationen werden normalerweise am besten auf GPUs ausgeführt, und das ist umständlich, wenn Sie Ihr Modell im Feld einsetzen wollen).\r\n",
        "\r\n",
        "Quantisierung bedeutet, diese Zahlendarstellung zu ändern. Mit der Float16-Quantisierung kann man z. B. Teile des Modells vom Float32- in das Float16-Format konvertieren - das reduziert die Modellgröße um etwa 50 %, ohne viel Leistung zu verlieren. Andere Ansätze erlauben die Quantisierung in das int8-Format (möglicherweise mit einem ziemlichen Performance-Verlust, aber einem 4-fachen Größengewinn) oder in ein kombiniertes int8/int16-Format (das Beste aus beiden Welten). Glücklicherweise ist es auch möglich, Ihr Modell quantisierungssensitiv zu machen, was bedeutet, dass es die Quantisierung während des Trainings simuliert, so dass sich die Schichten bereits an den durch die Quantisierung entstehenden Leistungsverlust anpassen können.\r\n",
        "\r\n",
        "Kurz gesagt, nachdem das Modell beschnitten wurde - d. h. von nicht beitragenden Gewichten befreit wurde - können wir nachträglich Quantisierung hinzufügen. Sie sollte das Modell auf zusammengesetzte Weise noch kleiner machen: Eine 2,35-fache Größenreduktion sollte theoretisch bei Verwendung der int8-Quantisierung eine 4 x 2,35 = 9,4-fache Größenreduktion bedeuten!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYQ9FkYQukHT"
      },
      "source": [
        "# Hinzufügen von Quantisierung zu unserem Keras-Beispiel\r\n",
        "Schauen wir uns nun an, wie wir Quantisierung zu einem beschnittenen TensorFlow-Modell hinzufügen können. Genauer gesagt, fügen wir eine dynamische Bereichsquantisierung hinzu, die die Gewichte quantisiert, aber nicht unbedingt die Modellaktivierungen.\r\n",
        "Dieser Konverter konvertiert Ihr TensorFlow-Modell in ein TensorFlow-Lite-Äquivalent, gegen das die Quantisierung laufen wird. Das Konvertieren des Modells in ein Lite-Modell erlaubt uns, einen Modell-Optimierer zu spezifizieren - in unserem Fall DEFAULT oder Dynamic Range Quantization. Schließlich konvertieren() wir das Modell:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4gnSP5F4GTYj",
        "outputId": "630a2f25-f964-4558-941f-eee14e038075"
      },
      "source": [
        "# Convert into TFLite model and convert with DEFAULT (dynamic range) quantization\r\n",
        "stripped_model = tfmot.sparsity.keras.strip_pruning(model_for_pruning)\r\n",
        "converter = tensorflow.lite.TFLiteConverter.from_keras_model(stripped_model)\r\n",
        "converter.optimizations = [tensorflow.lite.Optimize.DEFAULT]\r\n",
        "tflite_model = converter.convert()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpobwfs74q/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7ttKDT_u-JB"
      },
      "source": [
        "Beachten Sie, dass wir zuerst die Pruning-Wrapper aus dem Modell entfernen müssen, indem wir ein stripped_model erstellen. Wenn die Quantisierung des Modells abgeschlossen ist, können wir es speichern und seine Größe ausdrucken, um zu sehen, wie sehr sich die Situation verbessert hat:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4TwqyLFqu-0Q",
        "outputId": "52b22df3-023c-4750-f34e-6f1fbde741bb"
      },
      "source": [
        "# Save quantized model\r\n",
        "_, quantized_and_pruned_tflite_file = tempfile.mkstemp('.tflite')\r\n",
        "\r\n",
        "with open(quantized_and_pruned_tflite_file, 'wb') as f:\r\n",
        "  f.write(tflite_model)\r\n",
        "  \r\n",
        "# Additional details\r\n",
        "print(\"Size of gzipped pruned and quantized TFlite model: %.2f bytes\" % (get_gzipped_model_size(quantized_and_pruned_tflite_file)))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of gzipped pruned and quantized TFlite model: 190696.00 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcBlzVGDvCwy"
      },
      "source": [
        "...Bedeutung:\r\n",
        "\r\n",
        "Größenverbesserung Original -> Pruning: 2,35x\r\n",
        "\r\n",
        "Größenverbesserung Pruning -> Quantisierung: 3.64x\r\n",
        "\r\n",
        "Gesamtgrößenverbesserung Pruning + Quantisierung: 8.58x\r\n",
        "Fast 9 mal kleiner! "
      ]
    }
  ]
}